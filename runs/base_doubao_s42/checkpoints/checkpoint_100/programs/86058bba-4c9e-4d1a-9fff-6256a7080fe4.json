{"id": "86058bba-4c9e-4d1a-9fff-6256a7080fe4", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balanced search: global exploration + local exploitation\n        if np.random.rand() < 0.2:  # 20% local exploration\n            # Explore locally with adaptive step size that decreases over time\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            x = np.clip(best_x + np.random.normal(0, step_size), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, step_size), bounds[0], bounds[1])\n        else:  # 80% global exploration\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "bf09aaa6-c379-429d-bf8c-4bc3c1dd1694", "generation": 4, "timestamp": 1755422150.2477937, "iteration_found": 73, "metrics": {"runs_successfully": 1.0, "value_score": 0.9991497485606784, "distance_score": 0.9798899262390212, "overall_score": 0.997852109184703, "standard_deviation_score": 0.9829832987181, "speed_score": 1.0, "reliability_score": 1.0, "combined_score": 0.9892605459235149, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 3 lines with 4 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9991099509365003, "distance_score": 0.9792878940456907, "overall_score": 0.9978528170041876, "standard_deviation_score": 0.9841241963858542, "speed_score": 1.0, "reliability_score": 1.0, "combined_score": 0.9892640850209378, "success_rate": 1.0}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert coder helping to improve programs through evolution.", "user": "# Current Program Information\n- Current performance metrics: - runs_successfully: 1.0000\n- value_score: 0.9991\n- distance_score: 0.9793\n- overall_score: 0.9979\n- standard_deviation_score: 0.9841\n- speed_score: 1.0000\n- reliability_score: 1.0000\n- combined_score: 0.9893\n- success_rate: 1.0000\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, value_score: 0.9991, distance_score: 0.9793, overall_score: 0.9979, standard_deviation_score: 0.9841, speed_score: 1.0000, reliability_score: 1.0000, combined_score: 0.9893, success_rate: 1.0000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, value_score: 0.9995, distance_score: 0.9887, overall_score: 0.9988, standard_deviation_score: 0.9918, speed_score: 1.0000, reliability_score: 1.0000, combined_score: 0.9942, success_rate: 1.0000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, value_score: 0.9995, distance_score: 0.9875, overall_score: 0.9989, standard_deviation_score: 0.9939, speed_score: 1.0000, reliability_score: 1.0000, combined_score: 0.9943, success_rate: 1.0000\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9971)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point (combined for simplicity)\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balance global exploration and local exploitation\n        if np.random.rand() < 0.2:  # 20% chance of local exploration\n            # Explore locally around best solution with adaptive step size\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            x = best_x + np.random.normal(0, step_size)\n            y = best_y + np.random.normal(0, step_size)\n            # Keep within bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        else:  # 80% chance of global random search\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9995), Performs well on distance_score (0.9875), Performs well on overall_score (0.9989), Performs well on standard_deviation_score (0.9939), Performs well on speed_score (1.0000), Performs well on reliability_score (1.0000), Performs well on combined_score (0.9943), Performs well on success_rate (1.0000)\n\n\n### Program 2 (Score: 0.9970)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search: balance global exploration and local exploitation\n        if np.random.rand() < 0.2:  # 20% chance of local exploration\n            # Explore locally around best solution with decreasing step size\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            x = best_x + np.random.normal(0, step_size)\n            y = best_y + np.random.normal(0, step_size)\n            # Keep within bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        else:  # 80% chance of global random search\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9995), Performs well on distance_score (0.9887), Performs well on overall_score (0.9988), Performs well on standard_deviation_score (0.9918), Performs well on speed_score (1.0000), Performs well on reliability_score (1.0000), Performs well on combined_score (0.9942), Performs well on success_rate (1.0000)\n\n\n### Program 3 (Score: 0.9944)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balanced search: global exploration + local exploitation\n        if np.random.rand() < 0.2:  # 20% local exploration\n            # Explore around best solution with small perturbations\n            x = np.clip(best_x + np.random.normal(0, 0.3), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, 0.3), bounds[0], bounds[1])\n        else:  # 80% global exploration\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9991), Performs well on distance_score (0.9793), Performs well on overall_score (0.9979), Performs well on standard_deviation_score (0.9841), Performs well on speed_score (1.0000), Performs well on reliability_score (1.0000), Performs well on combined_score (0.9893), Performs well on success_rate (1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.9882)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A hybrid random search algorithm that combines global exploration with local refinement.\n    It performs random search globally with occasional local exploration around promising points.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random starting point within the search bounds\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n    search_width = bounds[1] - bounds[0]  # Calculate search space width\n\n    for _ in range(iterations):\n        # Adaptive search: explore locally with 30% probability, globally with 70%\n        if np.random.random() < 0.3:\n            # Local exploration around current best with decreasing step size\n            step_size = search_width * (1 - _/iterations) / 10\n            x = best_x + np.random.normal(0, step_size)\n            y = best_y + np.random.normal(0, step_size)\n            x, y = np.clip(x, bounds[0], bounds[1]), np.clip(y, bounds[0], bounds[1])\n        else:\n            # Global exploration - full random search\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n\n### Program D2 (Score: 0.9923)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point (combined for simplicity)\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balance exploration and exploitation\n        if np.random.random() < 0.8:  # 80% chance to exploit around current best\n            # Local search with small perturbations\n            x = np.clip(best_x + np.random.normal(0, 0.5), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, 0.5), bounds[0], bounds[1])\n        else:  # 20% chance to explore new areas\n            # Global random search\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.9971, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point (combined for simplicity)\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balance global exploration and local exploitation\n        if np.random.rand() < 0.2:  # 20% chance of local exploration\n            # Explore locally around best solution with adaptive step size\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            x = best_x + np.random.normal(0, step_size)\n            y = best_y + np.random.normal(0, step_size)\n            # Keep within bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        else:  # 80% chance of global random search\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 8 lines with 17 lines, Excellent runs_successfully (1.000), Excellent value_score (1.000)\n\n\n### Inspiration 2 (Score: 0.9637, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_point = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_x, best_y = best_point\n    best_value = evaluate_function(*best_point)\n\n    for _ in range(iterations):\n        # Simple random search\n        x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 4 lines with 4 lines\nChange 2: Replace 4 lines with 3 lines, Excellent runs_successfully (1.000), Excellent value_score (0.977)\n\n\n### Inspiration 3 (Score: 0.9633, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    # Initialize with random point (combined for simplicity)\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        # Generate candidate point (combined for simplicity)\n        x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.975), Excellent overall_score (0.986)\n\n\n### Inspiration 4 (Score: 0.9923, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point (combined for simplicity)\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balance exploration and exploitation\n        if np.random.random() < 0.8:  # 80% chance to exploit around current best\n            # Local search with small perturbations\n            x = np.clip(best_x + np.random.normal(0, 0.5), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, 0.5), bounds[0], bounds[1])\n        else:  # 20% chance to explore new areas\n            # Global random search\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 2 lines with 2 lines\nChange 2: Replace 2 lines with 8 lines, Excellent runs_successfully (1.000), Excellent value_score (0.999)\n\n\n### Inspiration 5 (Score: 0.8765, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Random search optimization algorithm to find function minimum.\n    \n    Randomly samples points within specified bounds and keeps the best \n    solution found.\n    \n    Args:\n        iterations: Number of search iterations\n        bounds: Tuple of (min, max) defining search space boundaries\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random starting point\n    best_coords = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(*best_coords)\n\n    for _ in range(iterations):\n        # Generate random candidate solution\n        candidate_coords = np.random.uniform(bounds[0], bounds[1], size=2)\n        candidate_value = evaluate_function(*candidate_coords)\n\n        # Update best solution if improved\n        if candidate_value < best_value:\n            best_coords = candidate_coords\n            best_value = candidate_value\n\n    return best_coords[0], best_coords[1], best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 26 lines with 28 lines, Excellent runs_successfully (1.000), Excellent value_score (0.963)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balanced search: global exploration + local exploitation\n        if np.random.rand() < 0.2:  # 20% local exploration\n            # Explore around best solution with small perturbations\n            x = np.clip(best_x + np.random.normal(0, 0.3), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, 0.3), bounds[0], bounds[1])\n        else:  # 80% global exploration\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["```python\n<<<<<<< SEARCH\n            # Explore around best solution with small perturbations\n            x = np.clip(best_x + np.random.normal(0, 0.3), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, 0.3), bounds[0], bounds[1])\n=======\n            # Explore locally with adaptive step size that decreases over time\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            x = np.clip(best_x + np.random.normal(0, step_size), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, step_size), bounds[0], bounds[1])\n>>>>>>> REPLACE\n```\n\nThis change improves both performance and maintainability by:\n\n1. **Adding adaptive step size** - The step size starts larger (supporting exploration) and decreases over time (supporting exploitation), which matches the approach used in the top-performing programs.\n\n2. **Improving readability** - The comment clearly explains the purpose of the code, and the step size calculation makes the algorithm's behavior more explicit.\n\n3. **Increasing robustness** - By calculating step size relative to the bounds rather than using a fixed value (0.3), the algorithm works better across different problem scales.\n\n4. **Maintaining simplicity** - The change is minimal and focused, keeping the code easy to understand while providing meaningful improvement.\n\nThis adaptive approach should help recover the performance metrics that declined from earlier iterations while keeping the code clean and maintainable."]}}}