{"id": "d975feac-77fe-4d53-b91e-000ba08aaf6d", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random starting point\n    best_coords = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(*best_coords)\n\n    for _ in range(iterations):\n        # Balance exploration and exploitation\n        if np.random.rand() < 0.2:  # 20% local exploration\n            # Adaptive step size that decreases over time\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            candidate_coords = best_coords + np.random.normal(0, step_size, size=2)\n            candidate_coords = np.clip(candidate_coords, bounds[0], bounds[1])\n        else:  # 80% global exploration\n            candidate_coords = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        candidate_value = evaluate_function(*candidate_coords)\n\n        # Update best solution if improved\n        if candidate_value < best_value:\n            best_coords = candidate_coords\n            best_value = candidate_value\n\n    return best_coords[0], best_coords[1], best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "a26ca279-0564-4faa-b9fe-8626dd2666ba", "generation": 4, "timestamp": 1755422475.139734, "iteration_found": 99, "metrics": {"runs_successfully": 1.0, "value_score": 0.9995036436018065, "distance_score": 0.986977810026655, "overall_score": 0.9986266260947244, "standard_deviation_score": 0.9893231085183014, "speed_score": 1.0, "reliability_score": 1.0, "combined_score": 0.9931331304736218, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 9 lines with 16 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9546023043642798, "distance_score": 0.6300371443976367, "overall_score": 0.9528463837506903, "standard_deviation_score": 0.5480405584339022, "speed_score": 1.0, "reliability_score": 1.0, "combined_score": 0.7642319187534512, "success_rate": 1.0}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert coder helping to improve programs through evolution.", "user": "# Current Program Information\n- Current performance metrics: - runs_successfully: 1.0000\n- value_score: 0.9546\n- distance_score: 0.6300\n- overall_score: 0.9528\n- standard_deviation_score: 0.5480\n- speed_score: 1.0000\n- reliability_score: 1.0000\n- combined_score: 0.7642\n- success_rate: 1.0000\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: value_score, distance_score, overall_score, standard_deviation_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, value_score: 0.9991, distance_score: 0.9799, overall_score: 0.9979, standard_deviation_score: 0.9830, speed_score: 1.0000, reliability_score: 1.0000, combined_score: 0.9893, success_rate: 1.0000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, value_score: 0.9993, distance_score: 0.9835, overall_score: 0.9983, standard_deviation_score: 0.9879, speed_score: 1.0000, reliability_score: 1.0000, combined_score: 0.9916, success_rate: 1.0000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, value_score: 0.9995, distance_score: 0.9861, overall_score: 0.9986, standard_deviation_score: 0.9896, speed_score: 1.0000, reliability_score: 1.0000, combined_score: 0.9929, success_rate: 1.0000\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9963)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search: balance global exploration and local exploitation\n        if np.random.rand() < 0.2:  # 20% chance of local exploration\n            # Explore locally around best solution with decreasing step size\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            x = best_x + np.random.normal(0, step_size)\n            y = best_y + np.random.normal(0, step_size)\n            # Keep within bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        else:  # 80% chance of global random search\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9995), Performs well on distance_score (0.9861), Performs well on overall_score (0.9986), Performs well on standard_deviation_score (0.9896), Performs well on speed_score (1.0000), Performs well on reliability_score (1.0000), Performs well on combined_score (0.9929), Performs well on success_rate (1.0000)\n\n\n### Program 2 (Score: 0.9956)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balanced search: global exploration + local exploitation\n        if np.random.rand() < 0.2:  # 20% local exploration\n            # Explore locally with adaptive step size\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            step = np.random.normal(0, step_size, 2)\n            x, y = np.clip([best_x, best_y] + step, bounds[0], bounds[1])\n        else:  # 80% global exploration\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9993), Performs well on distance_score (0.9835), Performs well on overall_score (0.9983), Performs well on standard_deviation_score (0.9879), Performs well on speed_score (1.0000), Performs well on reliability_score (1.0000), Performs well on combined_score (0.9916), Performs well on success_rate (1.0000)\n\n\n### Program 3 (Score: 0.9943)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balanced search: global exploration + local exploitation\n        if np.random.rand() < 0.2:  # 20% local exploration\n            # Explore locally with adaptive step size that decreases over time\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            x = np.clip(best_x + np.random.normal(0, step_size), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, step_size), bounds[0], bounds[1])\n        else:  # 80% global exploration\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9991), Performs well on distance_score (0.9799), Performs well on overall_score (0.9979), Performs well on standard_deviation_score (0.9830), Performs well on speed_score (1.0000), Performs well on reliability_score (1.0000), Performs well on combined_score (0.9893), Performs well on success_rate (1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.9908)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A hybrid random search algorithm that combines global exploration with local refinement.\n    It performs random search globally with occasional local exploration around promising points.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balance exploration and exploitation (80% exploit, 20% explore)\n        if np.random.random() < 0.8:\n            # Local search with small perturbations\n            x = np.clip(best_x + np.random.normal(0, 0.5), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, 0.5), bounds[0], bounds[1])\n        else:\n            # Global exploration - full random search\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n\n### Program D2 (Score: 0.9938)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A hybrid search algorithm combining random exploration with local exploitation.\n    \n    This algorithm performs mostly random search with occasional local exploration\n    around the best found point to balance exploration and exploitation.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point (vectorized for simplicity)\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Hybrid search: mostly random with occasional local exploration\n        if np.random.rand() < 0.2:  # 20% chance for local exploration\n            # Explore around best found point with small random perturbation\n            x = np.clip(best_x + np.random.normal(0, 0.3), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, 0.3), bounds[0], bounds[1])\n        else:  # 80% chance for global random search\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.9963, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search: balance global exploration and local exploitation\n        if np.random.rand() < 0.2:  # 20% chance of local exploration\n            # Explore locally around best solution with decreasing step size\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            x = best_x + np.random.normal(0, step_size)\n            y = best_y + np.random.normal(0, step_size)\n            # Keep within bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        else:  # 80% chance of global random search\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 2 lines\nChange 2: Replace 9 lines with 17 lines, Excellent runs_successfully (1.000), Excellent value_score (0.999)\n\n\n### Inspiration 2 (Score: 0.9633, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 2 lines\nChange 2: Replace 3 lines with 2 lines, Excellent runs_successfully (1.000), Excellent value_score (0.974)\n\n\n### Inspiration 3 (Score: 0.8789, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point (vectorized for simplicity)\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search (vectorized for simplicity)\n        x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 2 lines\nChange 2: Replace 3 lines with 2 lines, Excellent runs_successfully (1.000), Excellent value_score (0.966)\n\n\n### Inspiration 4 (Score: 0.9705, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    # Initialize with random point (combined for simplicity)\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        # Generate candidate point (combined for simplicity)\n        x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 2 lines with 2 lines\nChange 2: Replace 2 lines with 2 lines, Excellent runs_successfully (1.000), Excellent value_score (0.984)\n\n\n### Inspiration 5 (Score: 0.8471, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point (simplified)\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Balance exploration and exploitation (80% exploit, 20% explore)\n        if np.random.random() < 0.8:  # Exploit: search around current best\n            x = np.clip(best_x + np.random.normal(0, 0.5), bounds[0], bounds[1])\n            y = np.clip(best_y + np.random.normal(0, 0.5), bounds[0], bounds[1])\n        else:  # Explore: search new areas globally\n            x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 8 lines with 12 lines, Excellent runs_successfully (1.000), Excellent value_score (0.956)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random starting point\n    best_coords = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(*best_coords)\n\n    for _ in range(iterations):\n        # Generate random candidate solution\n        candidate_coords = np.random.uniform(bounds[0], bounds[1], size=2)\n        candidate_value = evaluate_function(*candidate_coords)\n\n        # Update best solution if improved\n        if candidate_value < best_value:\n            best_coords = candidate_coords\n            best_value = candidate_value\n\n    return best_coords[0], best_coords[1], best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["# Recommended Improvements\n\nThe current program uses a simple random search which explains the low distance_score (0.6300) and standard_deviation_score (0.5480). The top-performing programs all use a hybrid approach combining global exploration with local exploitation. Here are targeted improvements:\n\n## 1. Add Local Exploration Around Best Solutions\n\n<<<<<<< SEARCH\n    for _ in range(iterations):\n        # Generate random candidate solution\n        candidate_coords = np.random.uniform(bounds[0], bounds[1], size=2)\n        candidate_value = evaluate_function(*candidate_coords)\n\n        # Update best solution if improved\n        if candidate_value < best_value:\n            best_coords = candidate_coords\n            best_value = candidate_value\n=======\n    for _ in range(iterations):\n        # Balance exploration and exploitation\n        if np.random.rand() < 0.2:  # 20% local exploration\n            # Adaptive step size that decreases over time\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations) * 0.1\n            candidate_coords = best_coords + np.random.normal(0, step_size, size=2)\n            candidate_coords = np.clip(candidate_coords, bounds[0], bounds[1])\n        else:  # 80% global exploration\n            candidate_coords = np.random.uniform(bounds[0], bounds[1], size=2)\n            \n        candidate_value = evaluate_function(*candidate_coords)\n\n        # Update best solution if improved\n        if candidate_value < best_value:\n            best_coords = candidate_coords\n            best_value = candidate_value\n>>>>>>> REPLACE\n\n## Explanation of Changes\n\nThis improvement reintroduces the hybrid search strategy from the top-performing programs while maintaining code simplicity:\n\n1. **Adaptive Exploration Strategy**: We spend 80% of iterations on global exploration (random search) and 20% on local exploitation (searching around the best solution found so far).\n\n2. **Decreasing Step Size**: The local exploration uses a step size that decreases over time:\n   - Starts relatively large to explore nearby regions\n   - Gets smaller in later iterations to refine the solution\n   - Prevents getting stuck in local minima while still allowing precise convergence\n\n3. **Boundary Handling**: The `np.clip` ensures we stay within the search bounds during local exploration.\n\nThese changes should significantly improve:\n- **distance_score**: By focusing search around promising areas\n- **value_score**: By finding better minima through local refinement\n- **standard_deviation_score**: By consistently finding good solutions rather than relying on random chance\n\nThe changes maintain the program's speed and reliability while addressing the key metrics showing regression."]}}}