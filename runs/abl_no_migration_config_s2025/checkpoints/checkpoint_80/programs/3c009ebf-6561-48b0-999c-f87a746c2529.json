{"id": "3c009ebf-6561-48b0-999c-f87a746c2529", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Adaptive search algorithm balancing global exploration and local exploitation.\n\n    Efficiently explores search space by starting with broad exploration and\n    gradually shifting to focused exploitation around promising solutions.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value) - coordinates and value of minimum found\n    \"\"\"\n    # Cache bounds values for readability and performance\n    lower_bound, upper_bound = bounds\n    \n    # Initialize with random point\n    best_x, best_y = np.random.uniform(lower_bound, upper_bound, size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search strategy: balance exploration and exploitation\n        # 30% global search after warm-up, 70% local search otherwise\n        if np.random.random() < 0.3 and _ > iterations//4:\n            # Global random search\n            x, y = np.random.uniform(lower_bound, upper_bound, 2)\n        else:\n            # Explore around best solution with decreasing step size\n            step_size = (upper_bound - lower_bound) * (1 - _/iterations)\n            x = best_x + np.random.normal(0, step_size/3)\n            y = best_y + np.random.normal(0, step_size/3)\n            x, y = np.clip([x, y], lower_bound, upper_bound)\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "48a49f64-51cf-4526-b0d2-b12d261e790f", "generation": 4, "timestamp": 1755446623.3549762, "iteration_found": 68, "metrics": {"runs_successfully": 1.0, "value_score": 0.9994260659288743, "distance_score": 0.985233705076605, "overall_score": 0.9984298277572959, "standard_deviation_score": 0.987591094672809, "speed_score": 1.0, "reliability_score": 1.0, "combined_score": 0.9921491387864796, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 2 lines\nChange 2: 'x, y = np.clip(x, lower_bound, upper_bound), np.clip(y, lower_bound, upper_bound)' to 'x, y = np.clip([x, y], lower_bound, upper_bound)'\nChange 3: 'x, y = np.random.uniform(lower_bound, upper_bound, size=2)' to 'x, y = np.random.uniform(lower_bound, upper_bound, 2)'", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9995355781200009, "distance_score": 0.9895568707625053, "overall_score": 0.9988935466656481, "standard_deviation_score": 0.9914268810968156, "speed_score": 1.0, "reliability_score": 1.0, "combined_score": 0.9944677333282405, "success_rate": 1.0}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert coder helping to improve programs through evolution.", "user": "# Current Program Information\n- Current performance metrics: - runs_successfully: 1.0000\n- value_score: 0.9995\n- distance_score: 0.9896\n- overall_score: 0.9989\n- standard_deviation_score: 0.9914\n- speed_score: 1.0000\n- reliability_score: 1.0000\n- combined_score: 0.9945\n- success_rate: 1.0000\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: distance_score, overall_score, standard_deviation_score, combined_score. Consider continuing with similar changes.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, value_score: 0.9992, distance_score: 0.9830, overall_score: 0.9982, standard_deviation_score: 0.9865, speed_score: 1.0000, reliability_score: 1.0000, combined_score: 0.9911, success_rate: 1.0000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, value_score: 0.9995, distance_score: 0.9884, overall_score: 0.9988, standard_deviation_score: 0.9913, speed_score: 1.0000, reliability_score: 1.0000, combined_score: 0.9940, success_rate: 1.0000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, value_score: 0.9995, distance_score: 0.9896, overall_score: 0.9989, standard_deviation_score: 0.9914, speed_score: 1.0000, reliability_score: 1.0000, combined_score: 0.9945, success_rate: 1.0000\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9971)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Adaptive search algorithm balancing global exploration and local exploitation.\n\n    Efficiently explores search space by starting with broad exploration and\n    gradually shifting to focused exploitation around promising solutions.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value) - coordinates and value of minimum found\n    \"\"\"\n    # Cache bounds values for readability and performance\n    lower_bound, upper_bound = bounds\n    \n    # Initialize with random point\n    best_x, best_y = np.random.uniform(lower_bound, upper_bound, size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search strategy: balance exploration and exploitation\n        # 30% local search after warm-up, 70% global search otherwise\n        if np.random.random() < 0.3 and _ > iterations//4:\n            # Global random search\n            x, y = np.random.uniform(lower_bound, upper_bound, size=2)\n        else:\n            # Explore around best solution with decreasing step size\n            step_size = (upper_bound - lower_bound) * (1 - _/iterations)\n            x = best_x + np.random.normal(0, step_size/3)\n            y = best_y + np.random.normal(0, step_size/3)\n            x, y = np.clip(x, lower_bound, upper_bound), np.clip(y, lower_bound, upper_bound)\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9995), Performs well on distance_score (0.9896), Performs well on overall_score (0.9989), Performs well on standard_deviation_score (0.9914), Performs well on speed_score (1.0000), Performs well on reliability_score (1.0000), Performs well on combined_score (0.9945), Performs well on success_rate (1.0000)\n\n\n### Program 2 (Score: 0.9969)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm for function minimization.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point using vectorized operation\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    # Cache bounds values for performance\n    lower_bound, upper_bound = bounds\n    \n    for _ in range(iterations):\n        # Adaptive exploration strategy: start with more exploration, shift to exploitation\n        exploration_ratio = max(0.3, 1.0 - _/iterations)  # Decreases from 1.0 to 0.3\n        if np.random.random() < exploration_ratio:  # Adaptive exploration\n            x, y = np.random.uniform(lower_bound, upper_bound, size=2)\n        else:  # 70% chance for local refinement around best solution\n            # Refine around best solution with decreasing step size\n            step_size = (upper_bound - lower_bound) * (1 - _/iterations)\n            x = best_x + np.random.normal(0, step_size/3)\n            y = best_y + np.random.normal(0, step_size/3)\n            x, y = np.clip([x, y], lower_bound, upper_bound)\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9995), Performs well on distance_score (0.9884), Performs well on overall_score (0.9988), Performs well on standard_deviation_score (0.9913), Performs well on speed_score (1.0000), Performs well on reliability_score (1.0000), Performs well on combined_score (0.9940), Performs well on success_rate (1.0000)\n\n\n### Program 3 (Score: 0.9953)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid search algorithm with adaptive exploration-exploitation balance.\n    \n    Combines local refinement around promising solutions with periodic global\n    exploration to efficiently find function minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value) - coordinates and value of minimum found\n    \"\"\"\n    # Cache bounds values for readability and performance\n    lower_bound, upper_bound = bounds\n    \n    # Initialize with random point\n    best_x, best_y = np.random.uniform(lower_bound, upper_bound, size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search strategy: balance exploration and exploitation\n        if np.random.random() < 0.3 and _ > iterations//4:  # 30% local search after warm-up\n            # Explore around best solution with decreasing step size\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations)\n            x = best_x + np.random.normal(0, step_size/3)\n            y = best_y + np.random.normal(0, step_size/3)\n            x, y = np.clip(x, bounds[0], bounds[1]), np.clip(y, bounds[0], bounds[1])\n        else:  # 70% global random search\n            x, y = np.random.uniform(*bounds, 2)\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9992), Performs well on distance_score (0.9830), Performs well on overall_score (0.9982), Performs well on standard_deviation_score (0.9865), Performs well on speed_score (1.0000), Performs well on reliability_score (1.0000), Performs well on combined_score (0.9911), Performs well on success_rate (1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.9667)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm for function minimization.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Generate candidate solution\n        x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        current_value = evaluate_function(x, y)\n\n        if current_value < best_value:\n            best_value = current_value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n\n### Program D2 (Score: 0.9916)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], 2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search strategy: explore locally around good solutions\n        if np.random.random() < 0.3 and _ > iterations//4:  # 30% local search after warm-up\n            # Explore around best solution with decreasing step size\n            step_size = (bounds[1]-bounds[0]) * (1 - _/iterations)\n            x = best_x + np.random.normal(0, step_size/3)\n            y = best_y + np.random.normal(0, step_size/3)\n            x, y = np.clip(x, bounds[0], bounds[1]), np.clip(y, bounds[0], bounds[1])\n        else:  # 70% global random search\n            x, y = np.random.uniform(bounds[0], bounds[1], 2)\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Adaptive search algorithm balancing global exploration and local exploitation.\n    \n    Starts with broad exploration and gradually shifts to focused exploitation \n    around promising solutions for efficient search space navigation.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value) - coordinates and value of minimum found\n    \"\"\"\n    # Initialize with a random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], 2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search strategy: balance exploration vs exploitation\n        # Start with mostly exploration, gradually shift to exploitation\n        exploration_ratio = max(0.3, 1.0 - _/iterations)  # Decreases from 1.0 to 0.3\n        \n        if np.random.random() < exploration_ratio:\n            # Global exploration: search entire bounds\n            x, y = np.random.uniform(bounds[0], bounds[1], 2)\n        else:\n            # Local exploitation: refine around best solution with decreasing step size\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations)\n            x, y = best_x + np.random.normal(0, step_size/3, 2), best_y + np.random.normal(0, step_size/3, 2)\n            x, y = np.clip(x, bounds[0], bounds[1]), np.clip(y, bounds[0], bounds[1])\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Alternative runs_successfully approach, NumPy-based implementation, Comprehensive implementation\n\n\n### Inspiration 2 (Score: 0.9953, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid search algorithm with adaptive exploration-exploitation balance.\n    \n    Combines local refinement around promising solutions with periodic global\n    exploration to efficiently find function minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value) - coordinates and value of minimum found\n    \"\"\"\n    # Cache bounds values for readability and performance\n    lower_bound, upper_bound = bounds\n    \n    # Initialize with random point\n    best_x, best_y = np.random.uniform(lower_bound, upper_bound, size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search strategy: balance exploration and exploitation\n        if np.random.random() < 0.3 and _ > iterations//4:  # 30% local search after warm-up\n            # Explore around best solution with decreasing step size\n            step_size = (bounds[1] - bounds[0]) * (1 - _/iterations)\n            x = best_x + np.random.normal(0, step_size/3)\n            y = best_y + np.random.normal(0, step_size/3)\n            x, y = np.clip(x, bounds[0], bounds[1]), np.clip(y, bounds[0], bounds[1])\n        else:  # 70% global random search\n            x, y = np.random.uniform(*bounds, 2)\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 8 lines with 9 lines\nChange 2: Replace 3 lines with 6 lines, Excellent runs_successfully (1.000), Excellent value_score (0.999)\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point\n    best_x, best_y = np.random.uniform(*bounds, 2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search: balance exploration and exploitation\n        if np.random.random() < 0.2:  # 20% chance to explore globally\n            x, y = np.random.uniform(*bounds, 2)\n        else:  # 80% chance to exploit by searching near best solution\n            x, y = best_x + np.random.normal(0, 0.3, 2), best_y + np.random.normal(0, 0.3, 2)\n            x, y = np.clip(x, bounds[0], bounds[1]), np.clip(y, bounds[0], bounds[1])\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 2 lines with 2 lines\nChange 2: Replace 3 lines with 8 lines, Alternative runs_successfully approach, NumPy-based implementation\n\n\n### Inspiration 4 (Score: 0.9602, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point and evaluate\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], 2)\n    best_value = evaluate_function(best_x, best_y)\n\n    # Search for better solutions\n    for _ in range(iterations):\n        x, y = np.random.uniform(bounds[0], bounds[1], 2)\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.972), Excellent overall_score (0.985)\n\n\n### Inspiration 5 (Score: 0.9430, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with random point\n    best_x, best_y = np.random.uniform(bounds[0], bounds[1], size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Generate new random candidate point\n        x, y = np.random.uniform(bounds[0], bounds[1], size=2)\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 2 lines with 2 lines\nChange 2: Replace 2 lines with 2 lines, Excellent runs_successfully (1.000), Excellent value_score (0.951)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Adaptive search algorithm balancing global exploration and local exploitation.\n\n    Efficiently explores search space by starting with broad exploration and\n    gradually shifting to focused exploitation around promising solutions.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value) - coordinates and value of minimum found\n    \"\"\"\n    # Cache bounds values for readability and performance\n    lower_bound, upper_bound = bounds\n    \n    # Initialize with random point\n    best_x, best_y = np.random.uniform(lower_bound, upper_bound, size=2)\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Adaptive search strategy: balance exploration and exploitation\n        # 30% local search after warm-up, 70% global search otherwise\n        if np.random.random() < 0.3 and _ > iterations//4:\n            # Global random search\n            x, y = np.random.uniform(lower_bound, upper_bound, size=2)\n        else:\n            # Explore around best solution with decreasing step size\n            step_size = (upper_bound - lower_bound) * (1 - _/iterations)\n            x = best_x + np.random.normal(0, step_size/3)\n            y = best_y + np.random.normal(0, step_size/3)\n            x, y = np.clip(x, lower_bound, upper_bound), np.clip(y, lower_bound, upper_bound)\n            \n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["# Code Improvements for Readability and Maintainability\n\nBased on the current performance metrics and the feedback to improve readability and maintainability, I'll suggest targeted simplifications that preserve the algorithm's effectiveness while making the code clearer.\n\n## 1. Clarify Search Strategy Comment\n\nThe current comment about the search strategy is misleading - the condition actually performs global search when met, not local search.\n\n<<<<<<< SEARCH\n        # Adaptive search strategy: balance exploration and exploitation\n        # 30% local search after warm-up, 70% global search otherwise\n=======\n        # Adaptive search strategy: balance exploration and exploitation\n        # 30% global search after warm-up, 70% local search otherwise\n>>>>>>> REPLACE\n\n## 2. Simplify Clipping Operation\n\nCombine the two separate `np.clip()` calls into a single line for better readability.\n\n<<<<<<< SEARCH\n            x, y = np.clip(x, lower_bound, upper_bound), np.clip(y, lower_bound, upper_bound)\n=======\n            x, y = np.clip([x, y], lower_bound, upper_bound)\n>>>>>>> REPLACE\n\n## 3. Simplify Random Point Generation\n\nUse more concise syntax for generating random points while maintaining the same functionality.\n\n<<<<<<< SEARCH\n            x, y = np.random.uniform(lower_bound, upper_bound, size=2)\n=======\n            x, y = np.random.uniform(lower_bound, upper_bound, 2)\n>>>>>>> REPLACE\n\nThese changes improve code clarity without altering the algorithm's behavior or performance characteristics. The core adaptive search strategy that balances exploration and exploitation remains intact, while the code becomes more maintainable and less prone to misinterpretation."]}}}